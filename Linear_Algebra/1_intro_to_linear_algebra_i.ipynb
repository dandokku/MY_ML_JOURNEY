{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TABLE OF CONTENTS\n",
    "\n",
    "- [Intro to Linear Algebra](#intro-to-linear-algebra)\n",
    "    - [Brief History of Algebra](#briefhistoryofalgebra )\n",
    "    - [Application of Algebra](#application-of-algebra)\n",
    "- [Tensors](#tensors)\n",
    "    - [Scalar Tensors](#scalar-tensors)\n",
    "        - [Scalars in Pytorch](#scalars-in-pytorch)\n",
    "        - [Scalars in TensorFlow](#scalars-in-tensorflow)\n",
    "    - [Vector Tensors](#rank-1-vector-tensor)\n",
    "        - [Vector Transposition](#vector-transposition)\n",
    "        - [Zero Vectors](#zero-vectors-null-vectors)\n",
    "        - [Norms](#norms)\n",
    "            - [$L^1$ Norm](#l1-norm)\n",
    "            - [$L^2$ Norm](#l2-norm)\n",
    "            - [Squared $L^2$ Norm](#squared-l2-norm)\n",
    "            - [Max Norm](#max-norm-or-norm)\n",
    "            - [Generalized $L^p$ Norm](#generalized-norm)\n",
    "        - [Unit Vectors](#unit-vectors)\n",
    "        - [Basic Vectors](#basic-vectors)\n",
    "        - [Orthogonal Vectors](#orthogonal-vectors)\n",
    "    - [Matrices](#matrices)\n",
    "    - [Generic Tensor Notations](#generic-tensor-notation)\n",
    "        - [Higher-Rank Tensors](#higher-rank-tensors)\n",
    "    - [Tensor Operations](#tensor-operations)\n",
    "        - [Tensor Transposition](#tensor-transposition)\n",
    "- [Matrix Properties](#matrix-properties)\n",
    "    - [Frobenius Norm](#frobenius-norm)\n",
    "    - [Matrix Multiplication](#matix-multiplication)\n",
    "        - [Symmetric Matrices](#symmetric-matrices)\n",
    "        - [Identity Matrices](#identity-matrices)\n",
    "    - [Matrix Inversion](#matix-inversion)\n",
    "    - [Diagonal Matrices](#diagonal-matrices)\n",
    "    - [Orthogonal Matrices](#orthogonal-matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# INTRO TO LINEAR ALGEBRA\n",
    "\n",
    "Linear algebra is a branch of mathematics dealing with vector spaces and linear mapping between them. It involves the studying of linear equation, matrices and so on.\n",
    "It is important in machine learning to build a strong foundation\n",
    "\n",
    "In linear algebra, you either have one solution, no solution or infinite solutions. In a given system: \n",
    "- There could be many equations\n",
    "- There could be many unknowns in each equation\n",
    "Linear algebra involves the solving for unknowns within a system of linear equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this problem for example: A sherif has a 180km/h car and a bank robber has a 150km/h car and a 5 mins headstart. How long does it take the sherif to catch the robber and what distance would they have travelled at that point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.5' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/USER/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 40, 1000) #Start, finish, n points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance travelled by robber:  d = 2.5t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_r = 2.5 * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance travelled by sherif: d = 3(t - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_s = 3 * (t-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a subplot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Setting plot title and axis labels\n",
    "plt.title(\"Robber no sabi drive, na why them catch am\")\n",
    "plt.xlabel(\"Time (mins)\")\n",
    "plt.ylabel(\"Distance (km)\")\n",
    "\n",
    "# Setting x and y axis limits\n",
    "ax.set_xlim([0, 40])\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "# Plotting the distance of the robber and sheriff over time\n",
    "ax.plot(t, d_r, c='green', label='robber')  # Robber's distance over time (green line)\n",
    "ax.plot(t, d_s, c='blue', label='sheriff')  # Sheriff's distance over time (blue line)\n",
    "\n",
    "# Adding vertical and horizontal dashed lines to indicate a specific event (e.g., capture)\n",
    "plt.axvline(x=30, color='red', linestyle='--')  # Vertical dashed line at x=30\n",
    "_ = plt.axhline(y=75, color='red', linestyle='--')  # Horizontal dashed line at y=75\n",
    "\n",
    "# Adding a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='briefhistoryofalgebra'></a>\n",
    "## BRIEF HISTORY OF ALGEBRA\n",
    "\n",
    "Its roots can be traced back to ancient civilizations like babylonians and eygptians. During the islamic golden age, a scholar name Al-Khwarizmi introduced systematic methods for solving linear and quadratic equations. The word \"algebra\" was derived from Al-Khwairizmi's work \"al-jabr\" which means Completion, it was based on his book: \"The Compendious Book on Calculation by Completion and Balancing \" . Algebra has become a fundamental part of mathematics with applications extending beyond the field itself. It serves as a powerful tool in physics, engineering, computer science, and, as previously mentioned, plays a vital role in contemporary fields like machine learning.\n",
    "\n",
    "In summary, the history of algebra is a rich tapestry woven through centuries and across civilizations, with each era contributing to the development and refinement of algebraic concepts and techniques. Al-Khwarizmi's foundational work and the subsequent efforts of mathematicians have paved the way for the algebraic principles we use today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPLICATION OF ALGEBRA\n",
    "\n",
    "- Solving for unknowns in Machine Learning Algorithms, including Deep Learning\n",
    "- Reducing dimensionality (e.g, Principal Component Anaylsis)\n",
    "- Ranking results (e.g, with eigenvector)\n",
    "- Recommenders (e.g, Singular Value Decomposition [SVD])\n",
    "- Natural Language Processing (e.g, SVD, Matrix factorisation)\n",
    "    - Topic Modelling\n",
    "    - Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSORS\n",
    "\n",
    "Tensors are the basic data structures that plays a crucial role in representing and processing data in machine learning and deep learning models. It could be considered as a container for numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIMENSIONS IN TENSORS\n",
    "\n",
    "Dimensions refer to the size or extent of the tensor along each axis. The number of dimensions is known as the rank of the tensor:\n",
    "\n",
    "- `Scalar (Rank-0 Tensor)[0D]`\n",
    "- `Vector (Rank-1 Tensor)[1D]`\n",
    "- `Matrix (Rank-2 Tensor)[2D]`\n",
    "- `Higher Dimensional Tensors(More than 2D)`\n",
    "\n",
    "Tensor shape specifies the number of elements along each dimensions. E.g, a tensor with shape (2, 3, 4) has three dimensions, with 2 elements in the first dimension, 3 in the second and 4 in the third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Tensors\n",
    "\n",
    "- No dimensions\n",
    "- Single Number\n",
    "- Denoted in lowercase or italics\n",
    "- Should be typed like other tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANK-0 Scalar tensor in base python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12\n",
    "print(x)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 12.2\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars in Pytorch\n",
    "\n",
    "You can create a scalar using the torch.tensor() function with no arguments or use a specific creation function like torch.zeros(1) or torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "scalar_tensor = torch.tensor(5)\n",
    "\n",
    "zeros_scalar = torch.zeros(0)\n",
    "\n",
    "ones_scalar = torch.ones(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# using tf.Variable\n",
    "x_tf = tf.Variable(25)\n",
    "x_tf\n",
    "x_tf.shape\n",
    "\n",
    "y_tf = tf_Variable(12)\n",
    "\n",
    "x_tf + y_tf\n",
    "\n",
    "tf_sum = tf.add(x_tf, y_tf)\n",
    "tf_sum.numpy()\n",
    "\n",
    "type(tf_sum.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANK-1 Vector Tensor\n",
    "\n",
    "- One dimensional array of numbers\n",
    "- Denoted in lowercase, italics, bold, etc\n",
    "- Arranged in an order, so elements can be accessed by its index e.g [x1, x2]\n",
    "- Representing a point in space:\n",
    "    - Vector os length two represents location in 2D matrix\n",
    "    - Length of three represents location in 3D cube\n",
    "    - Length of n represents location in n-dimensional tensor\n",
    "- Represents a magnitude and direction from origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([25, 2, 5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Transposition\n",
    "\n",
    "This involves swapping the rows and columns of a vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing a regular 1-D array has no effect...\n",
    "x_t = x.T\n",
    "x_t\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[25, 2, 5]])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But you can transpose a matrix with a dimension of length 1\n",
    "y_t = y.T\n",
    "print(y_t)\n",
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also transpose it back\n",
    "y_t.T\n",
    "y_t.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Vectors (Null vectors)\n",
    "\n",
    "In this vector, all components are zero. \n",
    "\n",
    "Take this example: Two men apply equal forces on a wooden box but in opposite directions, will the box move in any direction?\n",
    "     \n",
    "    Solution: When two vector quantities with equal magnitudes ace in opposite directions, the net vector quantity is always a zero vector (null vector). Since equal forces are being applied to the body, the net force acting on the bus will be zero, therefore the box will not move in any direction.\n",
    "\n",
    "- Have no effect if added to another vector\n",
    "- It has no length and does not point in any specific direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros(3)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors in Pytorch and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pt = torch.tensor([25, 5, 2])\n",
    "x_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf = tf.Variable([25, 5, 2])\n",
    "x_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "\n",
    "Norms are functions that quantify vector magnitude, basically they are used to measure the size or length of a vector. The norm of a vector is a measure of its distance from the origin in the vector space.\n",
    "Common norms include:\n",
    "\n",
    "- $L^1$ Norm (Manhattan Norm)\n",
    "- $L^2$ Norm (Euclidean Norm)\n",
    "- Squared $L^2$ Norm\n",
    "- Max Norm\n",
    "- Generalized $L^p$ Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L^1$ Norm\n",
    "\n",
    "This is also known are the *manhattan distance or Taxicab norm* . The notation for $L^1$ norm of a vector x is $$ \\| \\mathbf{x} \\|_1  $$ and the formula is $$ \\| \\mathbf{x} \\|_1 = |x_1| + |x_2| + \\ldots + |x_n| $$\n",
    "\n",
    "- It varies linearly at all locations whether near or far from origin\n",
    "- It is used whenever difference between zero and non-zero is key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_l1 = np.array([25, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the norm of the vector, you need to sum the absolute values of each the elements in the array\n",
    "np.abs(25) + np.abs(2) + np.abs(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Norm\n",
    "\n",
    "It is also known as *Euclidean* norm. The notation for it is $$ \\| \\mathbf{x} \\|_2  $$, but it is so common that it can be denoted as $$ \\| \\mathbf{x} \\| $$. This norm measures simple distance from origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n = np.array([25, 2, 5])\n",
    "print(type(x_n))\n",
    "\n",
    "x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the norm of a vector, square each of the individual elements in an array, sum them up and find the square root\n",
    "(25**2 + 2**2 + 5**2) ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy comes with a built-in method for running this\n",
    "np.linalg.norm(x_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squared L2 Norm\n",
    "\n",
    "- This is computationally cheaper to use than L2 Norm because:\n",
    "\n",
    "    - Squared $ L^2 $ norm simply equals $X^TX$ $$ \\| \\mathbf{X} \\|_{2}^{2} = \\sum_{i=1}^{n} X_{i}^{2} $$\n",
    "    - Derivative (used to train many Ml algorithms) of element x requires that element alone, whereas $L^2$ norm requires x vector\n",
    "- Downside is it grows slowly near origin so it can't be used if distinguishing between zero and near-zero is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(25**2 + 2**2 + 5**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(x_l1, x_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Norm or $L^\\infty$ Norm\n",
    "\n",
    "It returns the maximum value of each of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([np.abs(25), np.abs(2), np.abs(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized $L^p$ Norm\n",
    "\n",
    "- p must be:\n",
    "    - A real number\n",
    "    - Greater than or equal to one\n",
    "- Can derive $L^1$, $L^2$, and $L^\\infty$ norm formulae by substituting for p\n",
    "- Norms, particularly $L^1$ and $L^2$, used to regularize objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Vectors\n",
    "\n",
    "Unit vectors are vectors with a magnitude of 1. The unit vector of a non zero vector can be obtained by dividing that vector by its magnitude\n",
    "\n",
    "- Special case of vector where its length is equal to one\n",
    "- Technically, x_n is a unit vector with \"unit norm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Vectors\n",
    "\n",
    "- Can be scaled to represent any vector in a given vector space\n",
    "- Typically use unit vectors along axes of vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Vectors\n",
    "\n",
    "- x and y are orthogonal vectos if $x^Ty$ = 0\n",
    "- Are at $90\\degree$ angle to each other (assuming non-zero norms)\n",
    "- n-dimensional space has max n mutually orthogonal vector (assuming non-zero norms)\n",
    "- Orthogonal vectors are orthogonal and all have unit norm\n",
    "    - Basic vectors are an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array([1, 0])\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.array([0, 1])\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "- A matrices is a two dimensional array of numbers\n",
    "- Denoted in uppercase, italics and bold\n",
    "- Height is given priority ahead of width in notation\n",
    "- Individual scalar elements denoted in uppercase, italics only\n",
    "- Colon represents an entire row or column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m = np.array([[25, 2], [5, 26], [3, 7]])\n",
    "x_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Tensor Notation\n",
    "\n",
    "- Upper-case, bold, itatalics and sans serif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-Rank Tensors\n",
    "\n",
    "As an example, Rank 4 tensors are common for images, where each dimensions corresponds to:\n",
    "\n",
    "- Number of images in training batch, e.g: 32\n",
    "- Image height in pixels, e.g: 28\n",
    "- Image width in pixels, e.g: 28\n",
    "- Number of color channels, e.g: 3 for full-color images(RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_pt = torch.zeros([32, 28, 28, 3])\n",
    "images_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSOR OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Transposition\n",
    "\n",
    "- The easiest type of tensors to transpose is a 0 dimensional tensor (scalar). The transpose of a scalar is simply itself. eg: $X^T$ = X\n",
    "\n",
    "- Transposing a vector is simply converting row to column and vice versa\n",
    "\n",
    "- In a matrix transposition:\n",
    "    - Flip the axes over main diagonal such that: \n",
    "        $(X^T)_i,_j = X_j,_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mt = np.array([[25, 2], [5, 26], [3, 7]])\n",
    "x_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mt.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard Product\n",
    "\n",
    "This is also known as the element wise operation, it involves taking in two matrices of the same sizes and returning a single matrix by multiplying their corresponding elements. The mathematical notation is $A \\odot X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh = np.array([2, 3, 4])\n",
    "xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yh = np.array([3, 2, 1])\n",
    "yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh * yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh + yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "The dot product or scalar product takes two equal-length sequences of numbers and returns a single number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh\n",
    "yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(xh, yh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenius Norm\n",
    "\n",
    "- Analogous to $L^2$ norm of vector\n",
    "- Measures the size of matrix in terms of Euclidean distance\n",
    "    - It is the sum of the magnitude of all the vectors in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_x = np.array([[1, 2], [3, 4]])\n",
    "m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the frobenius norm, you can use this method\n",
    "np.linalg.norm(m_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Matrices\n",
    "\n",
    "Are special matrix cases with following properties:\n",
    "\n",
    "    - Sqaure (It has as many rows as its columns, in this case its 3 x 3)\n",
    "    - $X^T$ ( When you transpose, you will end up witht the same matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Matrices\n",
    "\n",
    "Are symmetric matrices where: \n",
    "- Every element along main diagonal is 1\n",
    "- All other elements are 0\n",
    "- Notation: $I_n$ where n = height (or width)\n",
    "- n-length vector unchanged if multiplied by $I_n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matix Inversion\n",
    "\n",
    "- It is a clever and convenient approach for solving linear equations\n",
    "- It is an alternative to manually solving with substitution or elimation\n",
    "\n",
    "- The matrix inverse of X is denoted as $X^-1$\n",
    "    - Satisfies: $X^-1X = I_n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Matrices\n",
    "\n",
    "- Nonzero elements along main diagonal and zero everywhere else\n",
    "- Identity matrix is a diagonal matrix example\n",
    "- If sqaure, denoted as diag(x) where x is vector of main-diagonal elements\n",
    "- Computationally efficient:\n",
    "    - Multiplication: diag(x)y = $x \\odot y$\n",
    "    - Inversion: $diag(x)^-1 = diag[1/x_1,...,1/x_n]^T$\n",
    "        - Can't divide by zero, so x can't include zero\n",
    "- Can be non-square and computation still efficient:\n",
    "    - If h > w, simply add zeros to product\n",
    "    - If w > h, remove elements from product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Matrices\n",
    "\n",
    "In orthogonal matrices, othonormal vectors:\n",
    "\n",
    "    - Make up all rows\n",
    "    - Make up all columns\n",
    "    \n",
    "\n",
    "- This means: $A^TA = AA^T = I$\n",
    "- Which also means: $A^T = A^-1I = A^-1$\n",
    "- Calculating $A^T$ is cheap, therefore so is calculating $A^-1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
